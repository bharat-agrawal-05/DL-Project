{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a688324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/teaching/Desktop/Grp_22/BiometricSEAL/videoseal\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd /home/teaching/Desktop/Grp_22/BiometricSEAL/videoseal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3692f6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teaching/anaconda3/envs/videoseal/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import omegaconf\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "from videoseal.utils.display import save_img\n",
    "from videoseal.utils import Timer\n",
    "from videoseal.evals.full import setup_model_from_checkpoint\n",
    "from videoseal.evals.metrics import bit_accuracy, psnr, ssim\n",
    "from videoseal.augmentation import Identity, JPEG\n",
    "from videoseal.modules.jnd import JND\n",
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e8ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_imgs = -1\n",
    "assets_dir = \"../train/\"\n",
    "output_dir = \"apnaDataset\"\n",
    "base_output_dir = \"outputs\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1eaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code for loadigng tensor from tensor.pt\n",
    "tensor = torch.load(\"tensors.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5195ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tensors.txt', 'a') as f:\n",
    "    for i in tensor:\n",
    "        f.write(str(i) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68073fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"output/checkpoint025.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd26ffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from output/checkpoint025.pth with message: <All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model = setup_model_from_checkpoint(ckpt_path)\n",
    "model.eval()\n",
    "model.compile()\n",
    "model.to(device)\n",
    "model.blender.scaling_w *= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8602b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(assets_dir) if f.endswith(\".png\") or f.endswith(\".jpg\")]\n",
    "files = [os.path.join(assets_dir, f) for f in files]\n",
    "# files = files[:]\n",
    "for file in tqdm(files, desc=f\"Processing Images\"):\n",
    "        # load image\n",
    "        imgs = Image.open(file, \"r\").convert(\"RGB\")  # keep only rgb channels\n",
    "        imgs = to_tensor(imgs).unsqueeze(0).float()\n",
    "\n",
    "        # Watermark embedding\n",
    "        # timer.start()\n",
    "        custom_msg = random.choice(tensor)\n",
    "        #get index of the tensor\n",
    "        ind = 0\n",
    "        for i in range(len(tensor)):\n",
    "            if torch.equal(custom_msg, tensor[i]):\n",
    "                ind = i\n",
    "                break\n",
    "            \n",
    "        outputs = model.embed(imgs, msgs = custom_msg, is_video=False, lowres_attenuation=True)\n",
    "        torch.cuda.synchronize()\n",
    "        # print(f\"embedding watermark  - took {timer.stop():.2f}s\")\n",
    "\n",
    "        # compute diff\n",
    "        imgs_w = outputs[\"imgs_w\"]  # b c h w\n",
    "        msgs = outputs[\"msgs\"]  # b k\n",
    "        diff = imgs_w - imgs\n",
    "        diff = 25 * diff.abs()\n",
    "\n",
    "        # save\n",
    "        # timer.start()\n",
    "        base_save_name = os.path.join(output_dir, os.path.basename(file).replace(\".png\", \"\"))\n",
    "        # print(base_save_name)\n",
    "        # save_img(imgs[0], f\"{base_save_name}_ori.png\")\n",
    "        save_img(imgs_w[0], f\"{base_save_name}_wm.png\")\n",
    "        # save_img(diff[0], f\"{base_save_name}_diff.png\")\n",
    "\n",
    "        with open(\"Dataset.txt\", 'a') as f:\n",
    "                f.write(f\"{base_save_name}_wm.png;{ind};\\n\")\n",
    "\n",
    "        # Metrics\n",
    "        imgs_aug = imgs_w\n",
    "        outputs = model.detect(imgs, is_video=False)\n",
    "        metrics = {\n",
    "            \"file\": file,\n",
    "            \"bit_accuracy\": bit_accuracy(\n",
    "                outputs[\"preds\"][:, 1:],\n",
    "                msgs\n",
    "            ).nanmean().item(),\n",
    "            \"psnr\": psnr(imgs_w, imgs).item(),\n",
    "            \"ssim\": ssim(imgs_w, imgs).item()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40674b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ecf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.load(\"./tensors.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6280257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(tensor[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7352b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image in tensor\n",
    "wm_img = Image.open(\"apnaDataset/sa_11188.jpg_wm.png\", \"r\").convert(\"RGB\")  # keep only rgb channels\n",
    "wm_img = to_tensor(wm_img).unsqueeze(0).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1165b8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 770])\n"
     ]
    }
   ],
   "source": [
    "print(wm_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dde2cd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 770])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_im = Image.open(\"../train/sa_11188.jpg\", \"r\").convert(\"RGB\")\n",
    "ori_im = to_tensor(ori_im).unsqueeze(0).float().to(device)\n",
    "ori_im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ca848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor([0.5457, 0.3809, 0.3267, 0.6044, 0.2729, 0.6024, 0.4957, 0.2257, 0.1810,\n",
    "         0.4398, 0.2844, 0.4610, 0.5749, 0.2714, 0.4818, 0.6489, 0.4618, 0.4516,\n",
    "         0.6742, 0.2588, 0.5161, 0.4734, 0.2816, 0.3328, 0.5475, 0.1698, 0.6490,\n",
    "         0.4243, 0.8023, 0.4102, 0.6116, 0.3908, 0.5433, 0.5134, 0.7281, 0.6355,\n",
    "         0.4217, 0.6171, 0.7323, 0.5519, 0.8825, 0.5006, 0.7277, 0.2345, 0.5104,\n",
    "         0.3111, 0.3415, 0.5947, 0.4191, 0.5248, 0.5995, 0.5193, 0.5391, 0.3411,\n",
    "         0.8264, 0.6773, 0.6044, 0.6403, 0.7414, 0.3697, 0.6506, 0.4235, 0.3757,\n",
    "         0.6038, 0.4386, 0.7143, 0.6640, 0.4485, 0.3084, 0.1317, 0.4364, 0.4075,\n",
    "         0.5763, 0.4766, 0.4714, 0.0781, 0.6287, 0.3901, 0.4174, 0.5017, 0.3572,\n",
    "         0.6354, 0.4958, 0.8214, 0.5654, 0.6796, 0.4098, 0.4678, 0.3324, 0.6267,\n",
    "         0.2417, 0.5854, 0.2926, 0.6071, 0.6794, 0.2375, 0.3143, 0.5138, 0.3482,\n",
    "         0.3525, 0.8601, 0.1500, 0.3864, 0.4100, 0.4920, 0.5449, 0.6572, 0.3407,\n",
    "         0.3961, 0.2868, 0.5889, 0.4995, 0.3970, 0.6622, 0.5342, 0.6001, 0.7509,\n",
    "         0.3797, 0.4014, 0.4548, 0.1398, 0.1208, 0.5474, 0.2866, 0.7947, 0.7861,\n",
    "         0.3184, 0.2924, 0.5212])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248d58dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589c6daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a, b):\n",
    "    import torch.nn.functional as F\n",
    "    a = a.view(1, -1)\n",
    "    b = b.view(1, -1)\n",
    "    cos = F.cosine_similarity(a, b)\n",
    "    return cos.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27c1ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8234018683433533\n",
      "0.7873243093490601\n",
      "0.8287986516952515\n",
      "0.8181370496749878\n",
      "0.781493067741394\n",
      "0.8249193429946899\n",
      "0.8327418565750122\n",
      "0.8461710214614868\n",
      "0.8308339715003967\n",
      "0.8441811203956604\n",
      "0.8323599100112915\n",
      "0.7941389083862305\n",
      "0.8211923837661743\n",
      "0.8554711937904358\n",
      "0.8202939033508301\n",
      "0.7995548844337463\n",
      "0.8642961978912354\n",
      "0.7939215302467346\n",
      "0.8037793040275574\n",
      "0.7946099638938904\n",
      "0.8510743975639343\n",
      "0.8285080194473267\n",
      "0.7949255704879761\n",
      "0.7996906638145447\n",
      "0.8130337595939636\n",
      "0.8097113370895386\n",
      "0.7858120799064636\n",
      "0.8095712661743164\n",
      "0.8045985698699951\n",
      "0.8316817283630371\n",
      "0.8374031186103821\n",
      "0.8243930339813232\n",
      "0.8464142680168152\n",
      "0.8295799493789673\n",
      "0.8304649591445923\n",
      "0.8458789587020874\n",
      "0.8277631998062134\n",
      "0.8117071986198425\n",
      "0.8289353847503662\n",
      "0.7973959445953369\n",
      "0.8088787794113159\n",
      "0.8365012407302856\n",
      "0.8212382197380066\n",
      "0.832080066204071\n",
      "0.796597957611084\n",
      "0.8325291275978088\n",
      "0.7974777817726135\n",
      "0.8203443884849548\n",
      "0.8073456287384033\n",
      "0.8093833923339844\n"
     ]
    }
   ],
   "source": [
    "tensors = torch.load(\"tensors.pt\")\n",
    "for i in tensors:\n",
    "    print(cos(i, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193b114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videoseal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
